{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "2JEcme1a9wBu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n"
      ],
      "metadata": {
        "id": "xj1BdamT9v_p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4SsPpLY9v9B",
        "outputId": "ac498423-7d1d-4532-ea54-dace9ad2d3e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-24 05:07:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-10-24 05:07:12 (99.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "gT54kblY9v6r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Create a sorted list of unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# Calculate the vocabulary size\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create a dictionary to map characters to their indices\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "\n",
        "# Create a dictionary to map indices to characters\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoding the data using the character-to-index mapping\n",
        "data = [stoi[c] for c in text]\n",
        "\n",
        "\n",
        "# I've added comments to explain each step and improved variable names for better clarity and readability."
      ],
      "metadata": {
        "id": "AbacT1-A9v4T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the split index for train and validation data\n",
        "split_ratio = 0.8\n",
        "split_index = int(split_ratio * len(data))\n",
        "\n",
        "# Create TensorFlow tensors for train and validation data\n",
        "train_data_tensor = tf.constant(data[:split_index], dtype=tf.int32)\n",
        "val_data_tensor = tf.constant(data[split_index:], dtype=tf.int32)"
      ],
      "metadata": {
        "id": "wIeHR8iL9v1v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data_tensor, batch_size, block_size):\n",
        "    # Generate random start indices for batches\n",
        "    start_indices = tf.random.uniform((batch_size,), 0, len(data_tensor) - block_size, dtype=tf.int64)\n",
        "\n",
        "    # Create input (x) and target (y) batches\n",
        "    x_batch = tf.stack([data_tensor[start:start + block_size] for start in start_indices])\n",
        "    y_batch = tf.stack([data_tensor[start + 1:start + block_size + 1] for start in start_indices])\n",
        "\n",
        "    return x_batch, y_batch\n"
      ],
      "metadata": {
        "id": "4cGcM_lB9vzY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        self.values = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.keys = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.queries = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.fc_out = layers.Dense(embed_size)\n",
        "\n",
        "    def call(self, values, keys, query):\n",
        "        N, seq_length, _ = query.shape\n",
        "        value_len, key_len = values.shape[1], keys.shape[1]\n",
        "\n",
        "        # Split embedding into self.head pieces\n",
        "        values = tf.reshape(values, (N, value_len, self.heads, self.head_dim))\n",
        "        keys = tf.reshape(keys, (N, key_len, self.heads, self.head_dim))\n",
        "        queries = tf.reshape(query, (N, seq_length, self.heads, self.head_dim))\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = tf.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
        "        attention = attention / tf.math.sqrt(float(self.head_dim))\n",
        "        attention = tf.nn.softmax(attention, axis=-1)\n",
        "\n",
        "        out = tf.einsum(\"nhql,nlhd->nqhd\", attention, values)\n",
        "        out = tf.reshape(out, (N, seq_length, self.embed_size))\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.feed_forward = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(forward_expansion * embed_size, activation=\"relu\"),\n",
        "                layers.Dense(embed_size),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, value, key, query):\n",
        "        attention = self.attention(value, key, query)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "xjggdgre9vwy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class BigramLanguageModel(keras.Model):\n",
        "    def __init__(self, vocab_size, embed_size, heads, n_layers, max_length, forward_expansion, dropout):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embedding = layers.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embedding = layers.Embedding(max_length, embed_size)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "        # Output fully connected layer\n",
        "        self.fc_out = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        # Generate positional encodings\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        out = self.embedding(x)\n",
        "        out += self.positional_embedding(positions)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            out = block(out, out, out)\n",
        "\n",
        "        # Apply dropout\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Output layer\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "t-AncT9F9vuL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BigramLanguageModel\n",
        "model = BigramLanguageModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_size=n_embd,\n",
        "    heads=n_head,\n",
        "    n_layers=n_layer,\n",
        "    max_length=block_size,\n",
        "    forward_expansion=n_embd * 4,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "# Create an Adam optimizer\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# List to store generated text\n",
        "generated_text = []\n",
        "\n",
        "# Training loop\n",
        "for iteration in range(max_iters):\n",
        "    # Get a batch of data\n",
        "    x_batch, y_batch = get_batch(train_data_tensor, batch_size, block_size)\n",
        "\n",
        "    # Calculate the loss and gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Apply gradients using the optimizer\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Print loss at specified intervals\n",
        "    if iteration % eval_interval == 0:\n",
        "        print(f\"Iteration {iteration}, Loss: {loss.numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RABj-tYh9vrm",
        "outputId": "4bf991d0-adc0-4c23-9fb8-d06fe0ea8e79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7824454d7f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7824454d7f40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.474418640136719\n",
            "Iteration 100, Loss: 3.346715211868286\n",
            "Iteration 200, Loss: 3.2920451164245605\n",
            "Iteration 300, Loss: 3.2201244831085205\n",
            "Iteration 400, Loss: 3.251173496246338\n",
            "Iteration 500, Loss: 3.2371301651000977\n",
            "Iteration 600, Loss: 3.2259039878845215\n",
            "Iteration 700, Loss: 3.2018816471099854\n",
            "Iteration 800, Loss: 3.001217842102051\n",
            "Iteration 900, Loss: 2.764075756072998\n",
            "Iteration 1000, Loss: 2.6083405017852783\n",
            "Iteration 1100, Loss: 2.532355546951294\n",
            "Iteration 1200, Loss: 2.545006275177002\n",
            "Iteration 1300, Loss: 2.563915967941284\n",
            "Iteration 1400, Loss: 2.474588394165039\n",
            "Iteration 1500, Loss: 2.484832525253296\n",
            "Iteration 1600, Loss: 2.4055845737457275\n",
            "Iteration 1700, Loss: 2.4342117309570312\n",
            "Iteration 1800, Loss: 2.3724796772003174\n",
            "Iteration 1900, Loss: 2.409709930419922\n",
            "Iteration 2000, Loss: 2.3014073371887207\n",
            "Iteration 2100, Loss: 2.3063302040100098\n",
            "Iteration 2200, Loss: 2.370326280593872\n",
            "Iteration 2300, Loss: 2.1841650009155273\n",
            "Iteration 2400, Loss: 2.403679370880127\n",
            "Iteration 2500, Loss: 2.20084810256958\n",
            "Iteration 2600, Loss: 2.2140634059906006\n",
            "Iteration 2700, Loss: 2.2451016902923584\n",
            "Iteration 2800, Loss: 2.169163227081299\n",
            "Iteration 2900, Loss: 2.2122950553894043\n",
            "Iteration 3000, Loss: 2.1893811225891113\n",
            "Iteration 3100, Loss: 2.1021957397460938\n",
            "Iteration 3200, Loss: 2.275585174560547\n",
            "Iteration 3300, Loss: 2.1117124557495117\n",
            "Iteration 3400, Loss: 2.1789371967315674\n",
            "Iteration 3500, Loss: 2.218174457550049\n",
            "Iteration 3600, Loss: 2.054966688156128\n",
            "Iteration 3700, Loss: 2.1545886993408203\n",
            "Iteration 3800, Loss: 2.257248640060425\n",
            "Iteration 3900, Loss: 2.188222885131836\n",
            "Iteration 4000, Loss: 2.0761680603027344\n",
            "Iteration 4100, Loss: 2.0141711235046387\n",
            "Iteration 4200, Loss: 2.1039247512817383\n",
            "Iteration 4300, Loss: 2.0323574542999268\n",
            "Iteration 4400, Loss: 2.092271089553833\n",
            "Iteration 4500, Loss: 2.1659317016601562\n",
            "Iteration 4600, Loss: 1.9669206142425537\n",
            "Iteration 4700, Loss: 2.092578649520874\n",
            "Iteration 4800, Loss: 2.027609348297119\n",
            "Iteration 4900, Loss: 2.027076482772827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, max_generate_length=2000):\n",
        "    # Convert start_string to tensor\n",
        "    input_eval = [stoi[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    generated_text = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(max_generate_length):\n",
        "        logits = model(input_eval)\n",
        "        # Use a multinomial distribution to predict the token returned by the model\n",
        "        predicted_id = tf.random.categorical(logits[:, 0, :], num_samples=1)[-1,0].numpy()\n",
        "\n",
        "\n",
        "        # Append the predicted token to the input string and the generated text\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        generated_text.append(itos[predicted_id])\n",
        "\n",
        "    return ''.join(generated_text)\n",
        "\n",
        "start_string = \" \"  # You can use a space, or any other starting token\n",
        "print(generate_text(model, start_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K76qVW0p9vot",
        "outputId": "39fa8fa5-b6e5-4f9b-a5c8-ce5d85bdaec5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-sjwssseooedcseeeeeeeeiiiia    iiaa iaa  iaeeesfmpe\n",
            "aaassss  oeeeeeeeep;cckiiaaa Aoredeeeeeeeeeee ----nedbtcuoenrvhss  a  iaa  A a  aahchscly ie a \n",
            "aoe i-dddeeeeoi--iiiiaaa\n",
            " -----whsia dmeechhcoeeeee ia\n",
            "aa a ia;;S,seee'neee\n",
            "i    itiieee iaaa ia\n",
            "aaaaeeeeeiiiecooeeeeene aAoeoo aaa i rti------- iCreettteeeeeedesnneee \n",
            " aei -ooa tt  aaoeeeeeeiiii---   \n",
            "iwsse  a A hsiia Aereieoech''uoeeede  eia:tttbuoee aeeeeeeeeeeeeedsdccue - iieeemhspse a i-hsssdcthsc aeeeiii Ao iaaa aaa eedde munell eei iesmcheeeeeii- A;GO i\n",
            " eeereeeeeee aadsssbu i-b i\n",
            " \n",
            "aaaa a idrrpeeeeeedchmbhshp -wh aaeeeeeeeeeeeeedvueiA\n",
            "a DN i ---aaa aoeeee ii   aa aoeeeeeec iii\n",
            " ee a eeeeddeeesvlbeeei---\n",
            " -Mysreeee Afschssplpueea  a aa trcu   -iidddbyy'eds\n",
            "eeeeedeeem edeei---ssvuoeelreeiiA\n",
            "aaaaaaaa\n",
            "a  ---------aeeeeeeed'oeeeeedii\n",
            "  ----EE aeeeeeeeee \n",
            ".';   eeeeeeei a A;uuoedeeee aheedssssmcchfues -- hss  ivreeeeeeee aoeeeessbsddeoeeeeeeeemiibeeeeneirpurnnsssvhhss  aa ia eee,hddd\n",
            "  aoeeessddc aaeeeeeedeeeeedsssssdchhssssiiiei---iwc  aaaaaag   dddje aaa  aeeemlu\n",
            "a -ned---hvrnneeeeeeeeeenedeeeiaeddcchhhhbbl    aeee aaeeeV;'oeeeedseeemluno a ae-----tt  aeeeeeeeiihslueeee --- a  oee de aeede aaAo;eechssseeeeeeeepseeeddssseiii  aaa \n",
            "aaa    i   io,eeiaeeeeeaaaaaltteedvhchdm i --- id------ aaaaShhsseeioeeeeedsuneexdeeeeeeeeeeenieeeeepfsscuoaeet -ss \n",
            "a a iaaaeiii-i--- i- eeeeeede iKO\n",
            "aeeee o iaaeeds'oeiae aa ---------S oeereeeedreeeedeeeeeeeeeeiiia A;oxan aaa  -treediieeeeeeeeeii \n",
            "\n",
            "a ihhhssccl --sccii\n",
            " eeedpssseedlh  aa w --deeeeeedd aaee; ieeeeeegiaee  a   --\n",
            " - aaAeeeeeaeer\n",
            " 'nttttttchchwkeeesseeeeeeee aaaa iiiidssdeeeereedehssseeeeee  a-iieeeeeetiei--whhvcbeeeeedssssroeeed iaa \n",
            "a \n",
            "  e aaoeeed- \n",
            "mtted ed\n",
            " oee  Stii a en a    ----  mraa eeiEEE; - \n",
            " \n",
            "aaaa aaaaa eeeeeee in iie  eee neeeeeeiidsp hsssssddsss  -----------iiiiiiidsr Ao;;nccuoevscuoaa i--ttttttiaaoaaeeeecchsshcueeeedeeeeedsbi---aa;   ati iiiaoepsss  aeeeeedsssspssps a iia ---.nnre. iddchse aaa;; \n",
            "it a  \n",
            " iiiedsjssdcheedssseeeee a --\n",
            "aaaaeeeecchss aee A ii A;'oe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDGNdYrw9vmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nq_XghWB9vje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jt_kIYCv9vg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7sms1wDk9vej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GEtroQo9vcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8pAjxXp9vZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XNk9lgw9vXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lf9wIu2S9vUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWdEoC-I9vRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z2HYpCi9vPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmPTdIxK9vM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJORmiPx9vJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5qicn-39vHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YwRK2xQ9vFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iz1BoZ3G9vCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4-mFQAW9vAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCn2SCRE9u9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuO2AIOs4_TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFW-6zno4_YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fO1EnYOD4_a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Si11aB3V4_dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6h5Jq_e4_f4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}